<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Tracking</title>
    <style>
        body {
            margin: 0;
            padding: 0;
            width: 100vw;
            height: 100vh;
            overflow: hidden;
            background-color: #000;
            font-family: monospace;
        }
        
        #videoElement {
            position: absolute;
            width: 100%;
            height: 100%;
            object-fit: cover;
            transform: scaleX(-1); /* Mirror video */
            opacity: 0; /* Hide video by default */
        }
        
        #canvasOutput {
            position: absolute;
            width: 100%;
            height: 100%;
            z-index: 5;
            transform: scaleX(-1); /* Mirror debug visualization to match the video */
        }
        
        #animationCanvas {
            position: absolute;
            width: 100%;
            height: 100%;
            z-index: 10;
            transform: scaleX(-1); /* Mirror animation to match the video */
        }
        
        #debugInfo {
            position: absolute;
            top: 10px;
            left: 10px;
            background-color: rgba(0, 0, 0, 0.7);
            color: #0f0;
            padding: 10px;
            font-family: monospace;
            border-radius: 5px;
            z-index: 100;
        }
        
        #controls {
            position: absolute;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            z-index: 100;
            display: flex;
            gap: 10px;
            background: rgba(0, 0, 0, 0.7);
            padding: 15px;
            border-radius: 10px;
        }
        
        button {
            padding: 10px 20px;
            cursor: pointer;
            background: #000;
            border: 1px solid #0f0;
            color: #0f0;
            border-radius: 4px;
            font-weight: bold;
            font-family: monospace;
            text-transform: uppercase;
        }
        
        button:hover {
            background: #0f0;
            color: #000;
        }
        
        button:disabled {
            background: #333;
            border-color: #555;
            color: #555;
            cursor: not-allowed;
        }
        
        #loadingText {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            background-color: #000;
            color: #0f0;
            z-index: 1000;
            font-size: 24px;
            font-family: monospace;
            text-transform: uppercase;
        }
    </style>
</head>
<body>
    <!-- Loading Screen -->
    <div id="loadingText">
        <div>Initializing Face Tracking</div>
        <div style="margin-top: 20px; font-size: 16px;">Loading system...</div>
    </div>
    
    <!-- Debug Info -->
    <div id="debugInfo">
        Status: Initializing face tracking...
    </div>
    
    <!-- Controls -->
    <div id="controls">
        <button id="startButton">Start Camera</button>
        <button id="stopButton" disabled>Stop Tracking</button>
        <button id="toggleVideoBtn">Show Video</button>
    </div>
    
    <!-- Video and Canvas for MediaPipe -->
    <video id="videoElement" playsinline autoplay></video>
    <canvas id="canvasOutput"></canvas>
    <canvas id="animationCanvas"></canvas>
    
    <!-- MediaPipe Libraries -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>
    
    <script>
        // Elements
        const elements = {
            video: document.getElementById('videoElement'),
            canvas: document.getElementById('canvasOutput'),
            ctx: document.getElementById('canvasOutput').getContext('2d'),
            animCanvas: document.getElementById('animationCanvas'),
            animCtx: document.getElementById('animationCanvas').getContext('2d'),
            debugInfo: document.getElementById('debugInfo'),
            loading: document.getElementById('loadingText'),
            startButton: document.getElementById('startButton'),
            stopButton: document.getElementById('stopButton'),
            toggleVideoBtn: document.getElementById('toggleVideoBtn')
        };
        
        // App state
        const state = {
            isTracking: false,
            isVideoVisible: false
        };
        
        // MediaPipe objects
        let camera = null;
        let faceMesh = null;
        
        // Eye landmarks
        // Using the center points of the irises for pupil tracking
        const LEFT_IRIS_CENTER = 468;  // Center point of left iris
        const RIGHT_IRIS_CENTER = 473;  // Center point of right iris
        
        // Initialize the application
        async function initApp() {
            // Set up canvases
            setupCanvas();
            
            // Initialize tracking models
            try {
                await setupFaceMesh();
                elements.debugInfo.textContent = 'Status: Ready! Click "Start Camera" to begin.';
                elements.loading.style.display = 'none';
            } catch (error) {
                console.error('Error initializing tracking:', error);
                elements.debugInfo.textContent = `Error: ${error.message}`;
                elements.loading.style.display = 'none';
            }
            
            // Add event listeners
            elements.startButton.addEventListener('click', startCamera);
            elements.stopButton.addEventListener('click', stopTracking);
            elements.toggleVideoBtn.addEventListener('click', toggleVideo);
        }
        
        // Set up canvas sizes
        function setupCanvas() {
            const resizeCanvas = () => {
                const width = window.innerWidth;
                const height = window.innerHeight;
                
                // Set canvas dimensions
                elements.canvas.width = width;
                elements.canvas.height = height;
                elements.animCanvas.width = width;
                elements.animCanvas.height = height;
            };
            
            // Initial setup
            resizeCanvas();
            
            // Re-adjust on resize
            window.addEventListener('resize', resizeCanvas);
        }
        
        // Set up MediaPipe Face Mesh
        async function setupFaceMesh() {
            elements.debugInfo.textContent = 'Status: Loading MediaPipe models...';
            
            // Initialize Face Mesh
            faceMesh = new FaceMesh({
                locateFile: (file) => {
                    return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;
                }
            });
            
            // Set face mesh options
            faceMesh.setOptions({
                maxNumFaces: 1,
                refineLandmarks: true,  // Enable iris tracking
                minDetectionConfidence: 0.5,
                minTrackingConfidence: 0.5
            });
            
            // Set up result handler
            faceMesh.onResults(onFaceMeshResults);
            
            elements.debugInfo.textContent = 'Status: MediaPipe models loaded';
            return true;
        }
        
        // Start camera
        async function startCamera() {
            try {
                // Set up camera with MediaPipe Camera Utils
                camera = new Camera(elements.video, {
                    onFrame: async () => {
                        if (state.isTracking) {
                            await faceMesh.send({image: elements.video});
                        }
                    },
                    width: 1280,
                    height: 720
                });
                
                // Start camera
                await camera.start();
                
                // Update state and UI
                state.isTracking = true;
                elements.startButton.disabled = true;
                elements.stopButton.disabled = false;
                elements.debugInfo.textContent = 'Status: Tracking Active';
            } catch (error) {
                console.error('Error starting camera:', error);
                elements.debugInfo.textContent = `Camera error: ${error.message}`;
            }
        }
        
        // Stop tracking
        function stopTracking() {
            // Stop camera
            if (camera) {
                camera.stop();
            }
            
            // Update state and UI
            state.isTracking = false;
            elements.startButton.disabled = false;
            elements.stopButton.disabled = true;
            elements.debugInfo.textContent = 'Status: Tracking Stopped';
            
            // Clear canvases
            elements.ctx.clearRect(0, 0, elements.canvas.width, elements.canvas.height);
            elements.animCtx.clearRect(0, 0, elements.animCanvas.width, elements.animCanvas.height);
        }
        
        // Toggle video visibility
        function toggleVideo() {
            state.isVideoVisible = !state.isVideoVisible;
            elements.video.style.opacity = state.isVideoVisible ? '0.7' : '0';
            elements.toggleVideoBtn.textContent = state.isVideoVisible ? 'Hide Video' : 'Show Video';
        }
        
        // Draw eye pupils
        function drawPupil(ctx, landmarks, irisCenter, color) {
            // Get the center point coordinates
            const centerX = landmarks[irisCenter].x * ctx.canvas.width;
            const centerY = landmarks[irisCenter].y * ctx.canvas.height;
            
            // Draw a solid black circle for the pupil
            ctx.beginPath();
            ctx.arc(centerX, centerY, 5, 0, 2 * Math.PI);
            ctx.fillStyle = "#000000";
            ctx.fill();
            
            // Draw a colored outline for visibility
            ctx.beginPath();
            ctx.arc(centerX, centerY, 5, 0, 2 * Math.PI);
            ctx.strokeStyle = color;
            ctx.lineWidth = 1.5;
            ctx.stroke();
        }
        
        // Process face mesh results
        function onFaceMeshResults(results) {
            if (!state.isTracking) return;
            
            // Clear canvases
            elements.ctx.clearRect(0, 0, elements.canvas.width, elements.canvas.height);
            elements.animCtx.clearRect(0, 0, elements.animCanvas.width, elements.animCanvas.height);
            
            // If no face detected, update debug info
            if (!results.multiFaceLandmarks || results.multiFaceLandmarks.length === 0) {
                elements.debugInfo.textContent = 'No face detected';
                return;
            }
            
            // Draw face mesh on both canvases
            for (const landmarks of results.multiFaceLandmarks) {
                // Draw debug visualization on canvasOutput
                drawConnectors(elements.ctx, landmarks, FACEMESH_TESSELATION, 
                    {color: '#FFFFFF20', lineWidth: 1});
                drawConnectors(elements.ctx, landmarks, FACEMESH_RIGHT_EYE, 
                    {color: '#FFFFFF', lineWidth: 1});
                drawConnectors(elements.ctx, landmarks, FACEMESH_LEFT_EYE, 
                    {color: '#FFFFFF', lineWidth: 1});
                drawConnectors(elements.ctx, landmarks, FACEMESH_FACE_OVAL, 
                    {color: '#FFFFFF', lineWidth: 1});
                drawConnectors(elements.ctx, landmarks, FACEMESH_LIPS, 
                    {color: '#FFFFFF', lineWidth: 1});
                
                // Draw eye pupils on debug canvas
                drawPupil(elements.ctx, landmarks, LEFT_IRIS_CENTER, '#00FFFF');
                drawPupil(elements.ctx, landmarks, RIGHT_IRIS_CENTER, '#00FFFF');
                
                // Draw stylized face mesh on animationCanvas
                drawConnectors(elements.animCtx, landmarks, FACEMESH_TESSELATION, 
                    {color: '#FFFFFF40', lineWidth: 1.5});
                drawConnectors(elements.animCtx, landmarks, FACEMESH_RIGHT_EYE, 
                    {color: '#FFFFFF', lineWidth: 2});
                drawConnectors(elements.animCtx, landmarks, FACEMESH_LEFT_EYE, 
                    {color: '#FFFFFF', lineWidth: 2});
                drawConnectors(elements.animCtx, landmarks, FACEMESH_FACE_OVAL, 
                    {color: '#FFFFFF', lineWidth: 2.5});
                drawConnectors(elements.animCtx, landmarks, FACEMESH_LIPS, 
                    {color: '#FFFFFF', lineWidth: 2});
                
                // Draw eye pupils on animation canvas
                drawPupil(elements.animCtx, landmarks, LEFT_IRIS_CENTER, '#00FFFF');
                drawPupil(elements.animCtx, landmarks, RIGHT_IRIS_CENTER, '#00FFFF');
            }
            
            // Update debug info
            elements.debugInfo.textContent = 'Face detected! Tracking active.';
        }
        
        // Initialize the application on page load
        window.addEventListener('DOMContentLoaded', initApp);
    </script>
</body>
</html>